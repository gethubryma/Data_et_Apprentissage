# Data et Apprentissage

Ce projet regroupe une s√©rie de travaux pratiques autour de l‚Äô**apprentissage automatique**, de la **statistique**, de la **d√©tection d‚Äôanomalies** et du **clustering** .

## Structure du projet

* **data_et_apprentissage_1.ipynb**
  Contient :

  * la r√©gression robuste,
  * la mal√©diction de la dimension,
  * l‚Äô√©valuation des mod√®les,
  * l‚Äôapprentissage en ligne,
  * la r√©gularisation.

* **data_et_apprentissage_2.ipynb**
  Contient :

  * les mod√®les probabilistes,
  * la d√©tection d‚Äôanomalies,
  * le clustering incr√©mental,
  * la repond√©ration des distances,
  * les approches avanc√©es de clustering.

## Notebook 1 ‚Äî `data_et_apprentissage_1.ipynb`

### RANSAC et estimation robuste

* Impl√©mentation **from scratch** de l‚Äôalgorithme **RANSAC** pour la **r√©gression lin√©aire robuste**.
* Comparaison avec :

  * la **r√©gression lin√©aire classique**,
  * l‚Äôimpl√©mentation **RANSAC de scikit-learn**.
* √âtude de plusieurs sc√©narios de **bruit et d‚Äôoutliers** :

  * outliers sym√©triques,
  * bruit uniquement positif,
  * outliers localis√©s dans une zone sp√©cifique.
* Analyse qualitative et graphique de la **robustesse des mod√®les**.
* Impl√©mentation d‚Äôune **variante RANSAC bas√©e sur le vote des mod√®les**.
* Extension du principe RANSAC √† :

  * l‚Äôestimation robuste du **centre d‚Äôun nuage de points**,
  * le **clustering robuste** (type RANSAC-k-means).
* R√©daction d‚Äôun **pseudo-code g√©n√©ralis√©** pour RANSAC.

### Mal√©diction de la dimension

* √âtude empirique de la **curse of dimensionality** √† partir du livre
  *The Elements of Statistical Learning* (Section 2.5).
* Reproduction exp√©rimentale des r√©sultats illustr√©s dans les figures :

  * 2.6, 2.7, 2.8 et 2.9.
* Analyse de l‚Äôimpact de la **dimensionnalit√©** sur :

  * les distances,
  * la densit√© des donn√©es,
  * le comportement des m√©thodes d‚Äôapprentissage.

### √âvaluation des mod√®les et validation

* √âtude empirique du **ratio train/test** en classification.
* Analyse de l‚Äô√©volution des performances en fonction du ratio.
* Justification th√©orique via le **compromis biais‚Äìvariance**.
* Mise en place d‚Äôun sch√©ma **train / validation / test**.
* Optimisation des **hyperparam√®tres**.
* √âtude de la **cross-validation** dans le cadre des **s√©ries temporelles**, avec respect de l‚Äôordre temporel.

### Apprentissage en ligne (stream processing)

* Classification lin√©aire dans un contexte de **flux de donn√©es**.
* Donn√©es re√ßues s√©quentiellement, sans stockage global.
* D√©veloppement d‚Äôun mod√®le **frugal**, mis √† jour √† chaque instant.
* G√©n√©ration de donn√©es synth√©tiques via **distributions gaussiennes 2D**.
* Suivi des performances du mod√®le au cours du temps.
* Comparaison avec un classifieur entra√Æn√© en **batch**.

---

### R√©gression Ridge et Lasso

* Application de la **r√©gression Ridge** et de la **r√©gression Lasso**.
* Optimisation du param√®tre de r√©gularisation **Œ±** par **validation crois√©e**.
* Analyse de :

  * l‚Äô√©volution des **coefficients** en fonction de Œ±,
  * les **r√©sidus**,
  * le score de performance **R¬≤**.
* Discussion sur l‚Äôeffet de la r√©gularisation sur la **g√©n√©ralisation**.
* 
## Notebook 2 ‚Äî `data_et_apprentissage_2.ipynb`

### Mod√®le probabiliste ‚Äì Syst√®me de s√©curit√©

* Mod√©lisation probabiliste d‚Äôun syst√®me compos√© de :

  * une batterie,
  * deux capteurs,
  * une alarme.
* Construction du **mod√®le graphique**.
* Calcul des **probabilit√©s conjointes et marginales**.
* √âtude de l‚Äô**ind√©pendance conditionnelle** des variables.
* Inf√©rences bay√©siennes (posteriors).
* Validation empirique via **simulations Monte Carlo**.

---

### D√©tection d‚Äôanomalies : IF et LOF

* Impl√©mentation **from scratch** de **Local Outlier Factor (LOF)**.
* Comparaison avec l‚Äôimpl√©mentation **scikit-learn**.
* √âtude de **Isolation Forest (IF)** pour les outliers globaux.
* Proposition d‚Äôune m√©thode **hybride LOF + IF**.
* Comparaison exp√©rimentale entre :

  * LOF,
  * IF,
  * LOF+IF.
* Analyse des forces et limites de chaque approche.

### Clustering top-down incr√©mental

* √âtude des m√©thodes de **clustering hi√©rarchique top-down**.
* Impl√©mentation de :

  * **bisecting k-means**,
  * **global k-means (greedy)**,
  * **fast global k-means**,
  * **global++**.
* Analyse th√©orique des propri√©t√©s des solutions obtenues.
* Exp√©rimentations sur :

  * le clustering de couleurs,
  * des donn√©es synth√©tiques organis√©es en **grille 2D**.
* Extension des m√©thodes au cas des **GMM**, avec l‚Äôalgorithme **EM**.
* Discussion des difficult√©s du **bisecting** en contexte probabiliste.

### Repond√©ration des distances

* D√©finition de distances pond√©r√©es **$dist‚Ä≤(xi, xj)$**.
* Approches bas√©es sur :

  * **noyaux (RBF)**,
  * **clustering plat**,
  * **clustering hi√©rarchique**,
  * **clustering incr√©mental**.
* Construction de mesures de poids **wij** exploitant la structure des donn√©es.
* Exp√©rimentations sur donn√©es synth√©tiques.
* Analyse de l‚Äôimpact sur la structure des distances.

---

## üõ†Ô∏è Technologies utilis√©es

* **Python**
* **NumPy**, **SciPy**
* **scikit-learn**
* **Matplotlib / Seaborn**
* **Jupyter Notebook**

## Objectifs 

* Comprendre les **fondements th√©oriques** de l‚Äôapprentissage automatique.
* Impl√©menter des algorithmes **from scratch**.
* Comparer des approches classiques et avanc√©es.
* Analyser empiriquement les comportements des mod√®les.
* D√©velopper un raisonnement critique sur les m√©thodes de **data science**.

